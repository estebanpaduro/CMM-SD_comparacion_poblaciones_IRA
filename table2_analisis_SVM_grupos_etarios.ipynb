{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61de1472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "import duckdb\n",
    "#from mpl_toolkits import mplot3d \n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Dividing dataset into training and testing dataset and standarized the features\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e53993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "periodo = 365\n",
    "USE_CUMSUM = True\n",
    "grupo_etario_list=[\"Menores_1\",\"De_1_a_4\",\"De_5_a_14\",\"De_15_a_64\",\"De_65_y_mas\"]\n",
    "grupo_etario_legend=[\"Menores 1\",\"De 1 a 4\",\"De 5 a 14\",\"De 15 a 64\",\"De 65 y mas\"]\n",
    "#grupos_etarios = [\"Menores_1\",\"De_1_a_4\",\"De_5_a_14\",\"De_15_a_64\",\"De_65_y_mas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f37f8adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_filter = \"\"\"\n",
    "# FROM datos_urgencia_consolidados.parquet\n",
    "# where IdCausa in (3,4,5,6,10,11)\n",
    "# and CodigoDependencia is not null\n",
    "# --and GLOSATIPOESTABLECIMIENTO = 'Hospital'\n",
    "# and year(strptime(fecha, '%d/%m/%Y')) in (2017,2018,2019,2022,2023)\n",
    "# group by CodigoDependencia, fecha\n",
    "# \"\"\" + \"\"\"\n",
    "# having dayofyear(strptime(fecha, '%d/%m/%Y')) <= \"\"\"+ str(periodo) + \"\"\"\n",
    "# \"\"\"\n",
    "\n",
    "# df2 = duckdb.sql(\"\"\"\n",
    "# select year(strptime(fecha, '%d/%m/%Y')) as year\n",
    "#     , CodigoDependencia  as CodigoRegion\n",
    "#     , dayofyear(strptime(fecha, '%d/%m/%Y')) as nrodia\n",
    "#     , 'Menores_1' as grupo_etario\n",
    "#     , sum(Menores_1) as atenciones\n",
    "# \"\"\"+ common_filter + \"\"\"\n",
    "# UNION\n",
    "# select year(strptime(fecha, '%d/%m/%Y')) as year\n",
    "#     , CodigoDependencia  as CodigoRegion\n",
    "#     , dayofyear(strptime(fecha, '%d/%m/%Y')) as nrodia\n",
    "#     ,'De_1_a_4' as grupo_etario\n",
    "#     , sum(De_1_a_4) as atenciones\n",
    "# \"\"\"+ common_filter + \"\"\"\n",
    "# UNION\n",
    "# select year(strptime(fecha, '%d/%m/%Y')) as year\n",
    "#     , CodigoDependencia  as CodigoRegion\n",
    "#     , dayofyear(strptime(fecha, '%d/%m/%Y')) as nrodia\n",
    "#     , 'De_5_a_14' as grupo_etario\n",
    "#     , sum(De_5_a_14) as atenciones\n",
    "# \"\"\"+ common_filter + \"\"\"\n",
    "# UNION\n",
    "# select year(strptime(fecha, '%d/%m/%Y')) as year\n",
    "#     , CodigoDependencia  as CodigoRegion\n",
    "#     , dayofyear(strptime(fecha, '%d/%m/%Y')) as nrodia\n",
    "#     , 'De_15_a_64'  as grupo_etario\n",
    "#     , sum(De_15_a_64) as atenciones\n",
    "# \"\"\"+ common_filter + \"\"\"\n",
    "# UNION\n",
    "# select year(strptime(fecha, '%d/%m/%Y')) as year\n",
    "#     , CodigoDependencia  as CodigoRegion\n",
    "#     , dayofyear(strptime(fecha, '%d/%m/%Y')) as nrodia\n",
    "#     , 'De_65_y_mas' as grupo_etario\n",
    "#     , sum(De_65_y_mas) as atenciones\n",
    "# \"\"\"+ common_filter + \"\"\"\n",
    "# \"\"\").df()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "219f6ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from parquet file\n",
    "#number of days from the beginning of the year to consider in the analysis\n",
    "\n",
    "query_list = []\n",
    "\n",
    "for k, gr_etario in enumerate(grupo_etario_list):\n",
    "    query_list.append(\"\"\"\n",
    "select year(strptime(fecha, '%d/%m/%Y')) as year\n",
    "    , CodigoDependencia  as CodigoRegion\n",
    "    , dayofyear(strptime(fecha, '%d/%m/%Y')) as nrodia\n",
    "    , \"\"\" + \"'\" + gr_etario +\"'\"+ \"\"\" as grupo_etario\n",
    "    , sum(\"\"\"+ gr_etario + \"\"\") as atenciones\n",
    "FROM datos_urgencia_consolidados.parquet\n",
    "where IdCausa in (3,4,5,6,10,11)\n",
    "and CodigoDependencia is not null\n",
    "and year(strptime(fecha, '%d/%m/%Y')) in (2017,2018,2019,2022,2023)\n",
    "group by CodigoDependencia, fecha\n",
    "having dayofyear(strptime(fecha, '%d/%m/%Y')) <= \"\"\"+ str(periodo) )\n",
    "    if k +1< len(grupo_etario_list):\n",
    "        query_list.append(\"\"\" UNION \"\"\")\n",
    "\n",
    "df2 = duckdb.sql(''.join(query_list)).df()\n",
    "\n",
    "\n",
    "df2 = df2.sort_values(by=['year','CodigoRegion','grupo_etario','nrodia']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "233a737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the cumulative sum is taken on each of the classes, \n",
    "#is this case we group by \"year\", \"CodigoRegion\" and \"grupo_etario\"\n",
    "#the result is a dataframe with the \"nro_dia\" in the columns, \n",
    "#in which the cumulative information of the demand, scaled from 0 to 1 is stored\n",
    "groups = df2[[\"year\",\"CodigoRegion\",\"grupo_etario\"]].drop_duplicates()\n",
    "df_aux =df2.copy()\n",
    "for (index, year, region, grupo_edad) in groups.itertuples():\n",
    "    indicesToKeep = (df_aux.loc[:,'year'] == year) & (df_aux.loc[:,'CodigoRegion'] == region) & (df_aux.loc[:,'grupo_etario'] == grupo_edad)\n",
    "    if sum(indicesToKeep)>0:\n",
    "        if USE_CUMSUM:\n",
    "            df_aux.loc[indicesToKeep, 'atenciones'] = df_aux.loc[indicesToKeep, 'atenciones'].cumsum()\n",
    "            df_aux.loc[indicesToKeep, 'atenciones'] = df_aux.loc[indicesToKeep, 'atenciones'] / max(df_aux.loc[indicesToKeep, 'atenciones'])\n",
    "        else:\n",
    "            df_aux.loc[indicesToKeep, 'atenciones'] = df_aux.loc[indicesToKeep, 'atenciones'] / sum(df_aux.loc[indicesToKeep, 'atenciones'])\n",
    "df3 = df_aux.pivot(index=['year','CodigoRegion','grupo_etario'],columns= 'nrodia',values='atenciones').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8708d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analisis PCA\n",
    "# Separating out the features\n",
    "x_original = df3.loc[:, [k+1 for k in range(periodo)]].values\n",
    "\n",
    "# Separating out the target\n",
    "y = df3.loc[:,['CodigoRegion','grupo_etario']].values\n",
    "\n",
    "#Standardizing the features\n",
    "scaler = StandardScaler().fit(x_original)\n",
    "x = scaler.transform(x_original)\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "\n",
    "principalComponents = pca.fit_transform(x)\n",
    "\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['PC1', 'PC2', 'PC3','PC4'])\n",
    "finalDf = pd.concat([principalDf, df3[['CodigoRegion','grupo_etario']]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b771f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Menores 1</th>\n",
       "      <th>De 1 a 4</th>\n",
       "      <th>De 5 a 14</th>\n",
       "      <th>De 15 a 64</th>\n",
       "      <th>De 65 y mas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Menores 1</th>\n",
       "      <td>0.140923</td>\n",
       "      <td>0.433054</td>\n",
       "      <td>0.470906</td>\n",
       "      <td>0.447733</td>\n",
       "      <td>0.524429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>De 1 a 4</th>\n",
       "      <td>0.433054</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.839044</td>\n",
       "      <td>0.380244</td>\n",
       "      <td>0.385250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>De 5 a 14</th>\n",
       "      <td>0.470906</td>\n",
       "      <td>0.839044</td>\n",
       "      <td>0.527471</td>\n",
       "      <td>0.402762</td>\n",
       "      <td>0.450818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>De 15 a 64</th>\n",
       "      <td>0.447733</td>\n",
       "      <td>0.380244</td>\n",
       "      <td>0.402762</td>\n",
       "      <td>0.235119</td>\n",
       "      <td>0.773839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>De 65 y mas</th>\n",
       "      <td>0.524429</td>\n",
       "      <td>0.385250</td>\n",
       "      <td>0.450818</td>\n",
       "      <td>0.773839</td>\n",
       "      <td>0.573628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Menores 1  De 1 a 4  De 5 a 14  De 15 a 64  De 65 y mas\n",
       "Menores 1     0.140923  0.433054   0.470906    0.447733     0.524429\n",
       "De 1 a 4      0.433054  0.280656   0.839044    0.380244     0.385250\n",
       "De 5 a 14     0.470906  0.839044   0.527471    0.402762     0.450818\n",
       "De 15 a 64    0.447733  0.380244   0.402762    0.235119     0.773839\n",
       "De 65 y mas   0.524429  0.385250   0.450818    0.773839     0.573628"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test for a good group in the classification\n",
    "# here we test the different groupings to check if there \n",
    "# are others that make sense, since we test for binary classification \n",
    "#and we only have 5 classes it makes sense to consider the groupings\n",
    "# (1), (2), (3), (4), (5), (1,2), (1,3), (1,4), (1,5), (2,3), (2,4), (2,5),(3,4),(3,5),(4,5)\n",
    "#those can be presented in an array and they describe all the possibilities for binary classification\n",
    "\n",
    "maxiter = 1000\n",
    "resultados= np.zeros([maxiter,5,5])\n",
    "\n",
    "for iter in range(maxiter):\n",
    "    for group_indices in [[0], [1], [2], [3], [4], [0,1], [0,2], [0,3], [0,4], [1,2], [1,3], [1,4],[2,3],[2,4],[3,4]]:\n",
    "        # Declare the group we are trying to separate\n",
    "        targets = []\n",
    "        #for ind in set([0,1,2,3,4]) - set(group_indices):\n",
    "        for ind in group_indices:\n",
    "            if len(targets) == 0:\n",
    "                targets = (finalDf[[\"grupo_etario\"]]==grupo_etario_list[ind]).values.ravel()\n",
    "            else:\n",
    "                targets = targets | (finalDf[[\"grupo_etario\"]]==grupo_etario_list[ind]).values.ravel()\n",
    "        \n",
    "        X, y = finalDf[[\"PC1\",\"PC2\",\"PC3\"]].values , targets \n",
    "        # Split the dataset into a training and a testing set(20 percent)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "        #print (\"\\nTesting Dataset Shape:\", X_train.shape, y_train.shape)\n",
    "        \n",
    "        # Standarize the features\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # create the linear model SGDclassifier\n",
    "        linear_clf = SGDClassifier(loss=\"hinge\", alpha=0.000001, max_iter=1000)\n",
    "        # Train the classifier using fit() function\n",
    "        linear_clf.fit(X_train, y_train)\n",
    "        # Evaluate the result \n",
    "        y_train_pred = linear_clf.predict(X_train)\n",
    "        #precision_clasificador = metrics.f1_score(y_train, y_train_pred)\n",
    "        precision_clasificador = metrics.balanced accuracy_score(y_train, y_train_pred)\n",
    "        #presicion_clasificador = metrics.accuracy_score(y_train, y_train_pred)\n",
    "        if len(group_indices)==1:\n",
    "            resultados[iter, group_indices[0],group_indices[0]] = precision_clasificador\n",
    "        else:\n",
    "            resultados[iter, group_indices[0],group_indices[1]] = precision_clasificador\n",
    "            resultados[iter, group_indices[1],group_indices[0]] = resultados[iter,group_indices[0],group_indices[1]]\n",
    "#resultados.mean(0)\n",
    "pd.DataFrame(resultados.mean(0),columns=grupo_etario_legend, index = grupo_etario_legend)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
